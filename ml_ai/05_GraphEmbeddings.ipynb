{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Graph Embeddings\n",
    "\n",
    "In this notebook we'll learn how to use a graph embedding algorithm called DeepGL. We built an implementation of this in collaboration with [Pete Meltzer](mailto:p.meltzer@braintree.com) from [BrainTree Ltd](http://braintree.com/), an AI consultancy and research company.\n",
    "\n",
    "Embedding algorithms have become popular in recent years, and Airbnb have their own custom one for doing [listing recommendations](https://medium.com/airbnb-engineering/listing-embeddings-for-similar-listing-recommendations-and-real-time-personalization-in-search-601172f7603e).\n",
    "\n",
    "Embedding algorithms take a while to run on bigger datasets, and our implementation isn't yet optimised so we'll run it on a small dataset to gain an understanding about this approach.\n",
    "\n",
    "One of DeepGL's benchmark datasets is an email network, and our goal is to try and classify the emails based on the embeddings. \n",
    "\n",
    "Before you run this notebook create another database in your project and copy across the ml-models and Graph Algorithms plugins. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import manifold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Graph(\"bolt://localhost\", auth=(\"neo4j\", \"neo\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use an email dataset from SNAP, which contains relationships representing emails send between members of a European Research institution. Each node also has one community associated with it:\n",
    "\n",
    "<div>\n",
    "    <img align=\"left\" src=\"images/email_dataset.png\" alt=\"Graph Model\" width=\"500px\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can import the email dataset using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_list_file = \"https://github.com/meltzerpete/Embedding-Vis/raw/master/emails/emails.edgelist\"\n",
    "labels_file = \"https://github.com/meltzerpete/Embedding-Vis/raw/master/emails/emails.labels\"\n",
    "\n",
    "graph.run(\"CREATE CONSTRAINT ON (n:Node) ASSERT n.id IS UNIQUE\")\n",
    "    \n",
    "result = graph.run(\"\"\"\\\n",
    "    LOAD CSV FROM $edgelistFile AS row\n",
    "    FIELDTERMINATOR \" \"\n",
    "    MERGE (e1:Node {id: row[0]})\n",
    "    MERGE (e2:Node {id: row[1]})\n",
    "    MERGE (e1)-[:LINK]->(e2)\n",
    "    \"\"\", {\"edgelistFile\": edge_list_file})\n",
    "print(result.summary().counters)\n",
    "\n",
    "result = graph.run(\"\"\"\\\n",
    "    LOAD CSV FROM $labelsFile AS row\n",
    "    FIELDTERMINATOR \" \"\n",
    "    MATCH (e:Node {id: row[0]})\n",
    "    SET  e.label = toInteger(row[1])-1\n",
    "    \"\"\", {\"labelsFile\": labels_file})\n",
    "print(result.summary().counters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've done that we can run the embedding algorithm over the graph. We can optionally pass in several parameters to the algorithm:\n",
    "\n",
    "* `pruningLambda` - similarity cut off used when pruning features after each iteration. The lower this value is, the more aggressively features will be pruned  \n",
    "* `iterations` - the number of iterations that the algorithm should run. \n",
    "* `diffusions` - the number of rounds of diffusion of embeddings that should happen per iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_property_name  = \"deepglEmbedding\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruning_lambda = 0.6\n",
    "diffusions = 3\n",
    "iterations = 2\n",
    "\n",
    "params = {\n",
    "    \"writeProperty\": embedding_property_name,\n",
    "    \"pruningLambda\": pruning_lambda,\n",
    "    \"diffusions\": diffusions,\n",
    "    \"iterations\": iterations\n",
    "}\n",
    "result = graph.run(\"\"\"\n",
    "CALL embedding.deepgl(\"Node\", \"LINK\", {\n",
    "  pruningLambda: $pruningLambda,\n",
    "  diffusions: $diffusions,\n",
    "  iterations: $iterations,\n",
    "  writeProperty: $writeProperty\n",
    "})\n",
    "\"\"\", params)\n",
    "print(result.data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we're going to use these embeddings in a machine learning algorithm. Our classifier expects that label values increase incrementally from 0 so we'll create an `mlLabel` property on each node to store these labels. We're only going to do this for labels that have at least 50 items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = graph.run(\"\"\"\\\n",
    "MATCH (n:Node) \n",
    "WITH n.label as class, count(*) AS c\n",
    "ORDER BY c DESC\n",
    "WITH class WHERE c > 50\n",
    "WITH class ORDER BY class\n",
    "with collect(class) AS biggestClasses\n",
    "MATCH (p:Node) WHERE p.label IN biggestClasses\n",
    "SET p.mlLabel = apoc.coll.indexOf(biggestClasses, p.label)\n",
    "\"\"\")\n",
    "print(result.summary().counters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's write a query to get back the embeddings for each node along with its actual label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = graph.run(\"\"\"\\\n",
    "MATCH (p:Node) \n",
    "WHERE exists(p.mlLabel)\n",
    "RETURN p.`%s` AS embedding, p.mlLabel AS label, p.label as initialLabel\n",
    "ORDER BY label\n",
    "\"\"\" % embedding_property_name)\n",
    "\n",
    "df = result.to_data_frame()\n",
    "\n",
    "emb = df[\"embedding\"].apply(pd.Series).values\n",
    "labels = df[\"label\"].values\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a heatmap and scatterplot of our embeddings by running the following code. If two values in the matrix are similar they will be in dark blue. Since we've ordered the items in our DataFrame by label, we should expect there to be areas of blue around the diagonals of the matrix going from top left to bottom right:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap\n",
    "colours = ['r', 'g', 'b', 'black', 'y', 'orange']\n",
    "cols = pd.DataFrame(labels).apply(lambda x: colours[int(x)], axis=1).values\n",
    "\n",
    "dist = np.ndarray([len(emb), len(emb)])\n",
    "\n",
    "for i, e1 in enumerate(emb):\n",
    "    for j, e2 in enumerate(emb):\n",
    "        dist.itemset((i, j), np.linalg.norm(e1 - e2, 2))\n",
    "\n",
    "plt.imshow(dist)\n",
    "plt.axes().xaxis.tick_top()\n",
    "plt.xticks(np.arange(len(dist)), labels)\n",
    "plt.yticks(np.arange(len(dist)), labels)\n",
    "plt.show()\n",
    "\n",
    "# 2D Visualisation\n",
    "# from: https://baoilleach.blogspot.com/2014/01/convert-distance-matrix-to-2d.html\n",
    "adist = dist\n",
    "amax = np.amax(adist)\n",
    "adist /= amax\n",
    "\n",
    "mds = manifold.MDS(n_components=2, dissimilarity=\"precomputed\", random_state=6)\n",
    "results = mds.fit(adist)\n",
    "\n",
    "coords = results.embedding_\n",
    "\n",
    "plt.subplots_adjust(bottom=0.1)\n",
    "plt.scatter(\n",
    "    coords[:, 0], coords[:, 1], marker='o', c=cols\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the embeddings through a neural network. There are several different libraries that we could use here, but scikit-learn has a Multi Layer Perceptron classifier which is configurable in minimal lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(emb)\n",
    "y = labels\n",
    "\n",
    "X = StandardScaler().fit_transform(X)\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=.4, random_state=42)\n",
    "\n",
    "# can we tag the database with which nodes are in the test and training sets\n",
    "\n",
    "clf = MLPClassifier(solver='sgd',\n",
    "                    activation='tanh',\n",
    "                    learning_rate_init=0.001,\n",
    "                    alpha=1e-5,\n",
    "                    hidden_layer_sizes=(30, 30),\n",
    "                    max_iter=10000,\n",
    "                    batch_size=X.shape[0],\n",
    "                    random_state=0)\n",
    "\n",
    "display(clf.fit(train_x, train_y))\n",
    "\n",
    "mean_acc = clf.score(test_x, test_y)\n",
    "display(mean_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our classifier is getting an accuracy of just under 90%.\n",
    "\n",
    "Let's store our predictions in the database and then write a query to check the predictions against actual labels to make sure everything makes sense.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = graph.run(\"\"\"\\\n",
    "MATCH (n:Node) \n",
    "WHERE exists(n.mlLabel)\n",
    "RETURN n.mlLabel = n.prediction AS correct, count(*) AS count\n",
    "\"\"\")\n",
    "df = result.to_data_frame()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then write the following code to check the accuracy of our predictions as a %:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_predictions = df.loc[df[\"correct\"] == False][\"count\"].item()\n",
    "true_predictions = df.loc[df[\"correct\"] == True][\"count\"].item()\n",
    "\n",
    "true_predictions / (true_predictions + false_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is slightly lower than the score above, but remember that this is across the whole dataset rather than just the test set.\n",
    "\n",
    "Next we're going to compare the embeddings against each other and build an embedding similarity graph.\n",
    "\n",
    "First let's run the Cosine Similarity algorithm to find similar embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_query = \"\"\"\n",
    "MATCH (n:Node)\n",
    "WITH {item:id(n), weights: n.deepglEmbedding} as userData\n",
    "WITH collect(userData) as data\n",
    "CALL algo.similarity.cosine.stream(data, {concurrency: 1, topK: 3})\n",
    "YIELD item1, item2, similarity\n",
    "RETURN item1, item2, similarity\n",
    "ORDER BY item1 DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "result = graph.run(similarity_query).to_table()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use the `topK` variant to find the 3 most similar nodes for each node and create a `SIMILAR` relationship between nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_query = \"\"\"\n",
    "MATCH (n:Node)\n",
    "WITH {item:id(n), weights: n.deepglEmbedding} as userData\n",
    "WITH collect(userData) as data\n",
    "CALL algo.similarity.cosine(data, {concurrency: 1, topK: 3, similarityCutoff: 0.2, write: true})\n",
    "YIELD nodes, similarityPairs, write, writeRelationshipType, writeProperty, p50, p75, p90, p99, p999, p100\n",
    "RETURN nodes, similarityPairs, write, writeRelationshipType, writeProperty, p50, p75, p90, p99, p999, p100\n",
    "\"\"\"\n",
    "\n",
    "result = graph.run(similarity_query).to_table()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's write a query to find similar nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_similar_nodes_query = \"\"\"\n",
    "MATCH (n:Node {id: $id})-[similar:SIMILAR]->(other)\n",
    "RETURN n.id, n.label, other.id, other.label, similar.score\n",
    "\"\"\"\n",
    "    \n",
    "result = graph.run(find_similar_nodes_query, {\"id\": \"40\"}).to_data_frame()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would hope that similar nodes would have the same label which is the case here. Try changing the 'id' parameter to explore other nodes similar neighbors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
